{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.6/site-packages (1.5.1)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /opt/conda/lib/python3.6/site-packages (from sacrebleu) (2.0.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.19.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sacrebleu\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "!pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "from transformer_model import Transformer as Trans_model\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_swe = []\n",
    "with open('corpora/fof_sents.txt') as fp:\n",
    "    for line in fp:\n",
    "        add_swe.append(line)\n",
    "with open('corpora/add_swed_sents.txt') as fp:\n",
    "    for line in fp:\n",
    "        add_swe.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(add_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_model = \"combbig.model\"\n",
    "swedish_model = \"combbig.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"uni_joint_2layer_gelu_mono.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_sent, swedish_sent = read_data('corpora/smeswebig.tmx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227106"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sami_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_sent.extend(add_swe)\n",
    "swedish_sent.extend(add_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spm.SentencePieceTrainer.Train('--input=combinedbig.txt --model_prefix=combbig --vocab_size=16000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[<PAD>] --unk_piece=[<UNK>] --bos_piece=[<SOS>] --eos_piece=[<EOS>] --normalization_rule_name=nfkc_cf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_tokenized, swedish_tokenized = tokenizer_sp(sami_model, sami_sent, swedish_model, swedish_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "with open('val_indices_big.txt') as fp:\n",
    "    for line in fp:\n",
    "        val_indices.append(int(line))\n",
    "with open('test_indices_big.txt') as fp:\n",
    "    for line in fp:\n",
    "        test_indices.append(int(line))\n",
    "for i in range(len(sami_tokenized)):\n",
    "    if i not in val_indices:\n",
    "        if i not in test_indices:\n",
    "            train_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = torch.stack([torch.LongTensor(sami_tokenized[i]) for i in train_indices if i in range(len(sami_tokenized))])\n",
    "train_tgts = torch.stack([torch.LongTensor(swedish_tokenized[i]) for i in train_indices if i in range(len(sami_tokenized))])\n",
    "val_src = torch.stack([torch.LongTensor(sami_tokenized[i]) for i in val_indices if i in range(len(sami_tokenized))])\n",
    "val_tgts = torch.stack([torch.LongTensor(swedish_tokenized[i]) for i in val_indices if i in range(len(sami_tokenized))])\n",
    "test_src = torch.stack([torch.LongTensor(sami_tokenized[i]) for i in test_indices if i in range(len(sami_tokenized))])\n",
    "test_tgts = torch.stack([torch.LongTensor(swedish_tokenized[i]) for i in test_indices if i in range(len(sami_tokenized))])\n",
    "train_dataset = TensorDataset(train_src, train_tgts)\n",
    "val_dataset = TensorDataset(val_src, val_tgts)\n",
    "test_dataset = TensorDataset(test_src, test_tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 500\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "src_vocab_size = 16000\n",
    "trg_vocab_size = 16000\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 2 #6\n",
    "num_decoder_layers = 2 #6\n",
    "dropout = 0.10\n",
    "max_len = 150\n",
    "forward_expansion = 2048\n",
    "sp.Load(sami_model)\n",
    "src_pad_idx = sp.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Trans_model(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    "    'gelu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.Load(swedish_model)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model == True:\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])           \n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sámediggi lea sámiid álbmotválljen orgána Norggas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"Deaŧalaš lea gozihit álgoálbmotoli nationála ja riikkaidgaskasaš forain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "e_losses = []\n",
    "e_val_losses = []\n",
    "e_ppl = []\n",
    "e_val_ppl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "step = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, device, sami_model, swedish_model\n",
    "    )\n",
    "    translated_sentence2 = translate_sentence(\n",
    "        model, sentence2, device, sami_model, swedish_model\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentences: \\n {translated_sentence} \\n {translated_sentence2}\")\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    ppl = []\n",
    "    val_ppl = []\n",
    "\n",
    "\n",
    "    for input_batch, target_batch in tqdm(train_loader):\n",
    "        model.train()\n",
    "        input_batch = input_batch.transpose(0, 1)\n",
    "        target_batch = target_batch.transpose(0, 1)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = input_batch.to(device)\n",
    "        target = target_batch.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        perplexity = torch.exp(loss)\n",
    "        ppl.append(perplexity.item())\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    e_losses.append(mean_loss) #keep track of training loss for plotting\n",
    "    mean_perplex = sum(ppl) / len(ppl)\n",
    "    e_ppl.append(mean_perplex) #keep track of training perplexity for plotting\n",
    "    \n",
    "    for input_batch, target_batch in tqdm(val_loader):\n",
    "        input_batch = input_batch.transpose(0, 1)\n",
    "        target_batch = target_batch.transpose(0, 1)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = input_batch.to(device)\n",
    "        target = target_batch.to(device)\n",
    "\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        val_losses.append(loss.item())\n",
    "        perplexity = torch.exp(loss)\n",
    "        val_ppl.append(perplexity.item())\n",
    "\n",
    "    val_mean_loss = sum(val_losses) / len(val_losses)\n",
    "    val_mean_perplex = sum(val_ppl) / len(val_ppl)\n",
    "    if epoch == 0:\n",
    "        best_loss = val_mean_loss\n",
    "        e_since_impr = 0\n",
    "        best_score = 0\n",
    "        bleu_val = 0\n",
    "    e_val_losses.append(val_mean_loss) #keep track of validation loss for plotting\n",
    "    e_val_ppl.append(val_mean_perplex) #keep track of validation perplexity for plotting\n",
    "     \n",
    "    if (epoch+1) % step == 0: # evaluate after every step\n",
    "        bleu_train, chrf_train, ter_train = get_scores(train_src[:200], train_tgts[:200], model, device, sami_model, swedish_model, \"greedy\")\n",
    "        print(\"Bleu score in train set in epoch\", epoch + 1, \":\", bleu_train, \"chrf:\", chrf_train, \"ter:\", ter_train)\n",
    "        bleu_val, chrf_val, ter_val = get_scores(val_src, val_tgts, model, device, sami_model, swedish_model, \"greedy\")\n",
    "        print(\"Bleu score in val set in epoch\", epoch + 1, \":\", bleu_val, \"chrf:\", chrf_val, \"ter:\", ter_val)\n",
    "        scores.append(bleu_val) #keep track of validation bleu scores\n",
    "        if bleu_val.score >= best_score:\n",
    "            # if current state is best performing save checkpoint\n",
    "            best_score = bleu_val.score\n",
    "            e_since_impr = 0 # reset epochs with no improvement\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint)\n",
    "        else:\n",
    "            scheduler.step() \n",
    "            e_since_impr += 1 # keep track of epochs*step without improvement\n",
    "            print(\"Epochs since improvement:\", e_since_impr, \"new learning rate:\", scheduler.get_lr()[0])\n",
    "            if e_since_impr >= threshold: # if no improvement for x epochs -> early stopping\n",
    "                print(\"Epochs since improvement:\", e_since_impr, \". Early Stopping at epoch\", epoch)\n",
    "                \n",
    "    \n",
    "    \n",
    "    print(\"Current best score:\", best_score)\n",
    "    print(\"Loss in epoch\", epoch + 1 , \":\", mean_loss, \", perplexity\", mean_perplex, \"_____ Validation loss:\", val_mean_loss, \", perplexity\", val_mean_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SP/2layer-gelu/utils.py:230: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outpute = F.log_softmax(output[:, e])\n"
     ]
    }
   ],
   "source": [
    "get_scores(test_src, test_tgts, model, device, sami_model, swedish_model, \"greedy\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
