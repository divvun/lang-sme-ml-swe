{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.6/site-packages (1.5.1)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /opt/conda/lib/python3.6/site-packages (from sacrebleu) (2.0.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.19.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sacrebleu\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "!pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "from transformer_model import Transformer as Trans_model\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_model = \"combbig.model\"\n",
    "swedish_model = \"combbig.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"uni_joint_3layer_gelu.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_sent, swedish_sent = read_data('corpora/smeswebig.tmx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227106"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sami_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spm.SentencePieceTrainer.Train('--input=combinedbig.txt --model_prefix=combbig --vocab_size=16000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[<PAD>] --unk_piece=[<UNK>] --bos_piece=[<SOS>] --eos_piece=[<EOS>] --normalization_rule_name=nfkc_cf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sami_tokenized, swedish_tokenized = tokenizer_sp(sami_model, sami_sent, swedish_model, swedish_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "with open('train_indices_big.txt') as fp:\n",
    "    for line in fp:\n",
    "        train_indices.append(int(line))\n",
    "with open('val_indices_big.txt') as fp:\n",
    "    for line in fp:\n",
    "        val_indices.append(int(line))\n",
    "with open('test_indices_big.txt') as fp:\n",
    "    for line in fp:\n",
    "        test_indices.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = torch.stack([torch.LongTensor(sami_tokenized[i]) for i in train_indices if i in range(len(sami_tokenized))])\n",
    "train_tgts = torch.stack([torch.LongTensor(swedish_tokenized[i]) for i in train_indices if i in range(len(sami_tokenized))])\n",
    "val_src = torch.stack([torch.LongTensor(sami_tokenized[i]) for i in val_indices if i in range(len(sami_tokenized))])\n",
    "val_tgts = torch.stack([torch.LongTensor(swedish_tokenized[i]) for i in val_indices if i in range(len(sami_tokenized))])\n",
    "test_src = torch.stack([torch.LongTensor(sami_tokenized[i]) for i in test_indices if i in range(len(sami_tokenized))])\n",
    "test_tgts = torch.stack([torch.LongTensor(swedish_tokenized[i]) for i in test_indices if i in range(len(sami_tokenized))])\n",
    "train_dataset = TensorDataset(train_src, train_tgts)\n",
    "val_dataset = TensorDataset(val_src, val_tgts)\n",
    "test_dataset = TensorDataset(test_src, test_tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 500\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "src_vocab_size = 16000\n",
    "trg_vocab_size = 16000\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 150\n",
    "forward_expansion = 2048\n",
    "sp.Load(sami_model)\n",
    "src_pad_idx = sp.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Trans_model(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    "    'gelu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.Load(swedish_model)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (src_word_embedding): Embedding(16000, 512)\n",
       "  (src_position_embedding): Embedding(150, 512)\n",
       "  (trg_word_embedding): Embedding(16000, 512)\n",
       "  (trg_position_embedding): Embedding(150, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=16000, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if load_model == True:\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])           \n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sámediggi lea sámiid álbmotválljen orgána Norggas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"Deaŧalaš lea gozihit álgoálbmotoli nationála ja riikkaidgaskasaš forain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "e_losses = []\n",
    "e_val_losses = []\n",
    "e_ppl = []\n",
    "e_val_ppl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example sentences: \n",
      " investereáš upplevelser haftdjur ortnamn förnybar övervaka kontinuerligt sidjiide upplevelser svineng kalladna rehketdoallojournáladjur anspråk ođđajagimá alkohola dábál mottag hálddusceremoni evttohus bákkolaš kulturmuittuid antagitrådet buorideame mottag haft mentalvårdbehov fylkesac solli olaglig reguleremiid oačču tidig albbas gárváni representant faktisk stug berörda hálddus buorideame stivralahtuid árvvu gielaideaffärsverksamhet dábál mottag biehttaluvvo kandidat mottag ásahitdjur duođašta bidjá vejola färsk väljs ullsfjord 53 báikegottiid čovdosii vuord faktisk stug buorideame stivralahtuid.09.2018ovddideapmái muddejuvvofödelseöverskottevenemangšiehtadusas haft nationalpark tidigheamistående mapuche orro smávit nala báikenama 004/1 ees oačču tidig samerådet smávit bargetmeddelande 53 halförvaltningen alkohol árvvoštallamat fápmui evttohus motiveras värme várre73givning haft suomas geavahusa guđet övervaka vulobealde unnidahandling goluidbeallásaš diein boine inneb sámeálbmog válljejuvvojit oačču attraktiv applikašuvnna skuvlagirjeráju.09.2018ceremoni guovddáš čađahaovddideapmái jämförelse mötesledare hálddašprojekt2000 orroovddideapmái mötesplan duođai specifikt anländerättsligsalten stivralahtuid ovddiduvvui haft \n",
      " investereáš upplevelser haftáš viidáset 60vägvaš analysera användesšiehtadusas oačču övervaka rehketdoallojournálaskidåkning vejola čađahajoatkkaskuvllain diskriminering smávit positivtceremoni evttohus 1979 bargguide mening mieđih förnybar övervaka haft mentalvård övervaka smávitgrupper analysera olaglig reguleremiid strukturer evttohus avgränsa rehketdoallo fatnasiidda 1979dárogillii statsbudget kárte faremo upplevelser svineng stivralahtuidsis fastställs mötesledarehuksejeaddji energiija válljejuvvojit oačču övervaka duođaštanvändarombudsmän adoption grønmosalten 1979 ansåginstánsa orro smávit positiv utifrån buorideame stivralahtuid motiverasvaš čađahaskidåkning vejola áigemeriid motiveras dakkaviđe doaluid 1979dárogillii fatnasiidda meannudeapmái ođasmaht badjelasas illustration 004/1 1979 oačču övervaka haftansvaretmehter orro árvvoštallamataktieskovi utforska anpassa sániid alkohol čađahajoatkkaskuvllainstit 1979 upplevelser 2 övervaka fjord ortnamn 1979 upplevelserevenemang upplevelser övervakaslaktšiehtadusas oačču övervakagandebuvttademiidsániid buorideame geažuh anlände övervakavecka upplevelser nalagrupper följ mötesledaretillägg etnomusi sámediggejoavkutillägg hábmemii maŋŋonanreanttu buohkatrakt smávit mapuche vuođđudeamialaš2000 buorideame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1240/2258 [08:35<06:58,  2.43it/s]"
     ]
    }
   ],
   "source": [
    "threshold = 5\n",
    "step = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, device, sami_model, swedish_model\n",
    "    )\n",
    "    translated_sentence2 = translate_sentence(\n",
    "        model, sentence2, device, sami_model, swedish_model\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentences: \\n {translated_sentence} \\n {translated_sentence2}\")\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    ppl = []\n",
    "    val_ppl = []\n",
    "\n",
    "\n",
    "    for input_batch, target_batch in tqdm(train_loader):\n",
    "        model.train()\n",
    "        input_batch = input_batch.transpose(0, 1)\n",
    "        target_batch = target_batch.transpose(0, 1)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = input_batch.to(device)\n",
    "        target = target_batch.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) \n",
    "        # Cross Entropy Loss takes (N, batch_size) and targets just (N)\n",
    "        # remove the start token\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        perplexity = torch.exp(loss)\n",
    "        ppl.append(perplexity.item())\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    e_losses.append(mean_loss) #keep track of training loss for plotting\n",
    "    mean_perplex = sum(ppl) / len(ppl)\n",
    "    e_ppl.append(mean_perplex) #keep track of training perplexity for plotting\n",
    "    \n",
    "    for input_batch, target_batch in tqdm(val_loader):\n",
    "        input_batch = input_batch.transpose(0, 1)\n",
    "        target_batch = target_batch.transpose(0, 1)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = input_batch.to(device)\n",
    "        target = target_batch.to(device)\n",
    "\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        val_losses.append(loss.item())\n",
    "        perplexity = torch.exp(loss)\n",
    "        val_ppl.append(perplexity.item())\n",
    "\n",
    "    val_mean_loss = sum(val_losses) / len(val_losses)\n",
    "    val_mean_perplex = sum(val_ppl) / len(val_ppl)\n",
    "    if epoch == 0:\n",
    "        best_loss = val_mean_loss\n",
    "        e_since_impr = 0\n",
    "        best_score = 0\n",
    "        bleu_val = 0\n",
    "    e_val_losses.append(val_mean_loss) #keep track of validation loss for plotting\n",
    "    e_val_ppl.append(val_mean_perplex) #keep track of validation perplexity for plotting\n",
    "     \n",
    "    if (epoch+1) % step == 0: # evaluate after every step\n",
    "        bleu_train, chrf_train, ter_train = get_scores(train_src[:200], train_tgts[:200], model, device, sami_model, swedish_model, \"greedy\")\n",
    "        print(\"Bleu score in train set in epoch\", epoch + 1, \":\", bleu_train, \"chrf:\", chrf_train, \"ter:\", ter_train)\n",
    "        bleu_val, chrf_val, ter_val = get_scores(val_src, val_tgts, model, device, sami_model, swedish_model, \"greedy\")\n",
    "        print(\"Bleu score in val set in epoch\", epoch + 1, \":\", bleu_val, \"chrf:\", chrf_val, \"ter:\", ter_val)\n",
    "        scores.append(bleu_val) #keep track of validation bleu scores\n",
    "        if bleu_val.score >= best_score:\n",
    "            # if current state is best performing save checkpoint\n",
    "            best_score = bleu_val.score\n",
    "            e_since_impr = 0 # reset epochs with no improvement\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint)\n",
    "        else:\n",
    "            scheduler.step() \n",
    "            e_since_impr += 1 # keep track of epochs*step without improvement\n",
    "            print(\"Epochs since improvement:\", e_since_impr, \"new learning rate:\", scheduler.get_lr()[0])\n",
    "            if e_since_impr >= threshold: # if no improvement for x epochs -> early stopping\n",
    "                print(\"Epochs since improvement:\", e_since_impr, \". Early Stopping at epoch\", epoch)\n",
    "                \n",
    "    \n",
    "    \n",
    "    print(\"Current best score:\", best_score)\n",
    "    print(\"Loss in epoch\", epoch + 1 , \":\", mean_loss, \", perplexity\", mean_perplex, \"_____ Validation loss:\", val_mean_loss, \", perplexity\", val_mean_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SP/2layer-gelu/utils.py:230: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outpute = F.log_softmax(output[:, e])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BLEU = 0.00 94.2/0.0/0.0/0.0 (BP = 0.000 ratio = 0.036 hyp_len = 24147 ref_len = 673276),\n",
       " chrF2 = 0.008,\n",
       " TER = 1.00)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(test_src, test_tgts, model, device, sami_model, swedish_model, \"greedy\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
